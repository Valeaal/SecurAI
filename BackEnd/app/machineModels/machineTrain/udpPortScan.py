import joblib
import numpy as np
import pandas as pd
from sklearn.utils import resample
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense  # type: ignore
from tensorflow.keras.models import Sequential  # type: ignore
from tensorflow.keras.callbacks import EarlyStopping  # type: ignore

# â”€â”€ Cargar el dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
data = pd.read_csv('./app/machineModels/dataSetsOriginals/udpPortScan.csv')

# â”€â”€ Filtrar solo paquetes UDP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
udpData = data[data['prot'] == 17].copy()

# â”€â”€ Ordenar por timestamp â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
udpData.sort_values(by='#:unix_secs', inplace=True)

# â”€â”€ Feature: segundos desde el anterior paquete UDP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
udpData['segundosDesdeAnteriorPaquete'] = udpData['#:unix_secs'].diff().fillna(0)

# â”€â”€ Feature: nÃºmero de puertos Ãºnicos contactados por IP â”€â”€â”€â”€â”€â”€
puertosPorIP = udpData.groupby('srcaddr')['dstport'].nunique().reset_index()
puertosPorIP.rename(columns={'dstport': 'numeroPuertosPorIP'}, inplace=True)
udpData = udpData.merge(puertosPorIP, on='srcaddr', how='left')

# â”€â”€ Seleccionar columnas finales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
finalColumns = [
    'dpkts',
    'doctets',
    'srcaddr',
    'dstaddr',
    'dstport',
    'segundosDesdeAnteriorPaquete',
    'numeroPuertosPorIP',
    'Label'
]
udpData = udpData[finalColumns]

# â”€â”€ Seleccionar features para entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
features = [
    'dpkts',
    'doctets',
    'dstport',
    'segundosDesdeAnteriorPaquete',
    'numeroPuertosPorIP'
]
X = udpData[features]
y = udpData['Label']

# â”€â”€ Normalizar datos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# â”€â”€ Dividir en entrenamiento y prueba â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# â”€â”€ Definir modelo secuencial â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model = Sequential([
    Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(8, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# â”€â”€ Entrenar con EarlyStopping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
earlyStopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=64,
    callbacks=[earlyStopping],
    verbose=1
)

# â”€â”€ Evaluar el modelo en el conjunto de prueba â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print("\nReporte de clasificaciÃ³n en el conjunto de prueba:")
print(classification_report(y_test, y_pred, target_names=['Normal', 'Attack']))

# â”€â”€ Guardar el modelo y el escalador â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_path = './app/machineModels/models/udpPortScan.h5'
scaler_path = './app/machineModels/models/udpPortScan.pkl'
transformed_csv_path = './app/machineModels/dataSetsTransformed/udpPortScan.csv'

udpData.to_csv(transformed_csv_path, index=False)
print(f"ðŸ“‚ Dataset transformado guardado en: {transformed_csv_path}")

model.save(model_path)
joblib.dump(scaler, scaler_path)
print(f"âœ… Modelo guardado en: {model_path}")
print(f"âœ… Scaler guardado en: {scaler_path}")
